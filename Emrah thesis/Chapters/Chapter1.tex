% Chapter 1

\chapter{Introduction} % Main chapter title

\label{Chapter1} % For referencing the chapter elsewhere, use \ref{Chapter1} 

\lhead{Chapter 1. \emph{Introduction}} % This is for the header on each page - perhaps a shortened title

%----------------------------------------------------------------------------------------

\section{Motivation}
Finding the accurate information has been one of the main intellectual problems in human history. Different disciplines; from mathematics to philosophy, developed unique methods to obtain the information desired. In the case of computer science, the time spent for reaching the data is an important metric as the accuracy.  
Complexity of the search methods depend on the data structure as well as the size of the search set. In the notion O(\emph{n}), \emph{n} denotes the number of objects that are included the in the set. Table \ref{tabular:comparisonSearch} shows different runtimes for various data structures.  

\begin{table}
	\begin{center}
		\begin{tabular}{|c|c|p{4cm}|}
			\hline
			\textbf{Structure} & \textbf{Average } & \textbf{Worst Case} \\
			\hline
			Array& $O(n)$ & $O(n)$ \\
			\hline
			B-Tree & $O(log{}(n))$ & $O(log{}(n))$ \\
			\hline
			Binary Search Tree &  $O(log{}(n))$ & $O(n)$  \\
			\hline
			Hash Table & $O(1)$ & $O(n)$ \\
			\hline
		\end{tabular}
		\\
		\caption[]{Comparison of data structures in terms of search complexity.}
		\label{tabular:comparisonSearch}
	\end{center}
\end{table}

As it is easily observed, the set size has a direct effect on the runtime. Not only search engines but many applications today use databases at great sizes, forcing developers to optimize queries so that an acceptable runtime is preserved. And as stated above, having an optimum amount of data can have a positive effect on the runtime as having a well-structured search algorithm.
\subsection{Linked-data and RDF}
Various forms of data has been used for applications, retrieved from data sources of sorts. In parallel to the increase in internet coverage and integration of computers into daily life, many web applications have been produced to take over the processes that has been completed manually in person such as banking transactions, shopping, customer services, dispatching...etc, which required physical presence and paperwork. Keeping such records in databases instead of file cabinets seemed useful. In addition to the transfer of paperwork to digital media, use of ubiqituous computing had a huge impact on the  However the exponential growth in the data size of web led to problems as well as new ideas.
Table \ref{tabular:ipTraffic} shows the global internet traffic from 1992 to 2013 and Cisco's Visual Networking Index prediction for 2018. 

%\begin{figure}[htbp]
%	\centering
%	\includegraphics{Figures/cisco_data_per_month.jpg}
%	\rule{35em}{0.5pt}
%	\caption{Cisco's prediction on monthly data traffic over IP.}
%	\label{fig:ciscoDataPerMonth}
%\end{figure}

One of the most noticable problem that larger physical space needed to store the data produced and provide network to transfer the data. To reduce server costs, algorithms that require less space have been demanded. Data replications were also target to such resource planning. Same data of non-controversial facts have been repeated all over the web and seperate space has been allocated for each fragment. Example for such data can be capital cities, musical album names, timestamps of historical events...etc.

\begin{table}
	\begin{center}
		\begin{tabular}{|c|p{7cm}|}
			\hline
			\textbf{Year} & \textbf{Global Internet Traffic} \\
			\hline
			1992 & 100 GB Per Day \\
			\hline
			1997 & 100 GB Per Hour \\
			\hline
			2002 & 100 GBps \\
			\hline
			2007 & 2000 GBps  \\
			\hline
			2013 & 28,875 GBps \\
			\hline
			2018 & 50,000 GBps  \\
			\hline
		\end{tabular}
		\\
		\caption[]{The Cisco VNI Forecast Within Historical Context. Source: Cisco VNI, 2014}
		\label{tabular:ipTraffic}
	\end{center}
\end{table}

Even those who owned the data was willing to share what they had, it was avaiable for one or a few applications and that would be possible if only the other applications had access to the database or the data was shared over webservices, which required additional work and converting data into types like XML or JSON thus demanding additional resources. Servers would not be able to handle numerous calls. Computers, simply could not process the information on a web page efficiently since they did not understand it. A concept was required to ease the sharing of data without having to handle server requests at enormous scale, saving space by avoiding repetations and optimize resource usage by skipping conversion into different data types, simply offering a common form that could be processed by any computer.

The term semantic-web denotes to the "meaningful part of the web", which simply means the metadata of web pages and/or applications.
DuCharme describes semantic web in his book Learning Sparql; as a set of standards and best practices for sharing data and the semantics of that data over the Web for use by applications. \cite{Reference7}
Extracting the metadata would be useful to gather it and share among the applications that are also part of it. 
In 2006, W3C offered the concept of linked data, which basically is data that has links on it. The links enable computers to reach the address where the metadata is stored. Usage of the data is simply done by referencing a link to it.  
Or can be described as :
Technically, Linked Data refers to data published on the Web in such a way that it is machine-readable, its meaning is explicitly defined, it is linked to other external data sets, and can in turn be linked to from external data sets. \cite{Reference4}

The motivation for producing such structure has been listed by Klyne and Caroll as follows :
\begin{itemize}
	\item{Web metadata: providing information about Web resources and the systems that use them (e.g. content rating, capability descriptions, privacy preferences, etc.)}
	\item{Applications that require open rather than constrained information models (e.g. scheduling activities, describing organizational processes, annotation of Web resources, etc.)}
	\item{To do for machine processable information (application data) what the World Wide Web has done for hypertext: to allow data to be processed outside the particular environment in which it was created, in a fashion that can work at Internet scale.}
	\item{Interworking among applications: combining data from several applications to arrive at new information.}
	\item{Automated processing of Web information by software agents: the Web is moving from having just human-readable information to being a world-wide network of cooperating processes. RDF provides a world-wide lingua franca for these processes.}
\end{itemize} \cite{Reference5}

And to come up with a feasible solution to fulfil such requirements, Tim Berners-Lee offered a set of design principles for linked-data structure.

\begin{enumerate}
	\item{Use URIs as names for things. The uniqueness of URI's make it easier to describe the entities, hence the connection between them}
	\item{Use HTTP URIs so that people can look up those names. Different URI forms have been offered however such attempts would decrease the interoperability of linked data, which it is all about.}
	\item{When someone looks up a URI, provide useful information, using the standards (RDF*, SPARQL) }
	\item{ Include links to other URIs. so that they can discover more things. }
\end{enumerate} \cite{Reference6} 

Linked data enables the operations mentioned by providing a form of data that can be readable and processable by machines.
In order to have a better understanding of how linked data works, an introduction on RDF would be beneficial.

\textbf{RDF}

The notion RDF stands for Resource Description Framework, basically a data type for describing web resources. A simple RDF entity consists of three parts; object, predicate and subject hence called a "triple". Subject denotes the name of the resource and is of type URI (Uniform Resource Identifier), predicate is the property of that resource; pointing to an object. And object stores the value of that property, which belongs to the subject. An object can store different data types such as string, integer. date...etc. Or it can be another subject thus have its own URI.
 
Each triple can be visualised as a graph, which makes it easier to understand the RDF structure and why it is useful for linked-data movement. In such format, subjects and objects will be nodes and predicates are the edges that connects them. 
Suppose we would like to visualise the statement "The author of http://www.w3schools.com/rdf is Jan Egil Refsnes" [1] and store it as an RDF object.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=\textwidth]{Figures/rdf_example.png}
	\rule{35em}{0.5pt}
	\caption{Graph of the statement "The author of http://www.w3schools.com/rdf is Jan Egil Refsnes" [Generated at http://www.w3.org/RDF/Validator/]}
	\label{fig:rdfexample}
\end{figure}

Figure \ref{fig:rdfexample} is a graph representation of the statement mentioned. Subject is the node with the value http://www.w3schools.com/rdf, the property is shown as the edge author (which belongs to the namespace http://www.w3schools.com/rdf  therefore represented as http://www.w3schools.com/rdf/author) and object value is the "author" of the subject, Jan Egil Refsnes. 

RDF has an XML-like syntax, only with different classes and namespaces. The figure \ref{fig:rdfexample} can be represented as the following code : 

\begin{lstlisting}
	<?xml version="1.0"?>
	<rdf:RDF
		xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
		xmlns:si="http://www.w3schools.com/rdf/">
		<rdf:Description rdf:about="http://www.w3schools.com/rdf/">
			<si:author>Jan Egil Refsnes</si:author>
		</rdf:Description>
	</rdf:RDF> 
\end{lstlisting}

In 2000, Decker \& Harmelen discussed the advantages of RDF structure (while XML existed long before) for semantic web as: When it comes to 
Semantic Interoperability, 
RDF has significant advantages over XML: semantic units are given naturally through is object-attribute structure: all objects are independent entities. A domain model, defining objects and relationships of a domain of interest, can be represented naturally in RDF, so translation steps, as required when using XML, are not necessary. To find mappings between two RDF descriptions, techniques from Knowledge Representation are directly applicable.  \citep{Reference1}

\subsection{Linked Data Applications}
In this section we will have a glance over applications that are using linked-data and their database sizes. 

\textbf{BBCMedia}

\section{Objective}
In this master thesis, we will approach the problem of handling large linked-data datasets of diffeerent types. (RDF,N-Triples, Turtle, OWL...etc). Although large linked-data sets can consist of well structured graphs, the size of triples can reach up to billions and not all this data might be required, thus not have to be queried. As in other data structures, large linked-data sets also cause the problems of storage and performance. Runtimes for different linked-data analyse tools will be presented in the state of the art section. 

\begin{figure}[htbp]
	\centering
	\includegraphics[width=\textwidth]{Figures/lod-cloud.png}
	\rule{35em}{0.4pt}
	\caption{Latest state of Linked-data cloud \citep{Reference2}}
	\label{fig:}
\end{figure}

Today there are many projects available that are using graph-based databases as alternative to relational (SQL queried) or document-based databases. An example to such applications is DBPedia which is an important asset to Semantic Web
movement providing an alternative to Wikipedia offering an open data resource in RDF format.
However using data in such size is not that easy. DBPedia into a triple store can take up to 7 hours on a computer with standard hardware. Querying such entity during application creating process also will yield to decrease in efficiency.

In order to overcome the problem of dealing with huge data dumps, AKSW Research Group has come up with
a solution called RDFSlice, which basically creates slices from large datasets to form application specific knowledge. It also enables users to create slices from diferent datasources and combine them in a single file. Although RDFSlice is a platform-free tool, it currently can be executed through terminal or command prompt interpreter.

\begin{lstlisting} 
 java -jar rdfslice.jar -source <fileList>|<path> -patterns <graphPatterns> 
 -out <fileDest> -order <order> -debug <debugGraphSize> 
\end{lstlisting}

The parameter <graphPatterns> is either a SPARQL query or a graph pattern. Current release of RDFSlice doesn't provide a detailed feedback upon erroneous parameters. Thus users cannot identify which parameter is not accepted. Also complex patterns and class slicing requires advanced SPARQL knowledge. Plus RDFSlice doesn't store any information regarding the slice created except for the output file.
This thesis will focus on a software to support RDFSlice visually with additional attributes. 
A graphical user interface to;
\begin{itemize}
	\item{Analyse datasets by breaking them into subjects, predicates, objects and classes without using any endpoints }
	\item{Form SPARQL queries and patterns using entities of the dataset with no syntax errors.}
	\item{Create slices by executing RDFSlice without using terminal or command prompt}
	\item{Share and update created slices over a user-account based web portal}
\end{itemize}

